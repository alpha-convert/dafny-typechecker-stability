
\documentclass[sigplan,review,screen,anonymous]{acmart}
\usepackage[utf8]{inputenc}
\usepackage{mathpartir}

\setcopyright{none}
\definecolor{dkgreen}{HTML}{006329}
\newcommand{\comm}[3]{\textcolor{#1}{[#2: #3]}}
\newcommand{\jwc}[1]{\comm{dkgreen}{JWC}{#1}}

\include{dafnylst}


% ===== Authors =====

\author{Joseph W. Cutler}
\email{jwc@seas.upenn.edu}
\orcid{0000-0001-9399-9308} % chktex 8
\affiliation{%
  \institution{University of Pennsylvania}
  \city{Philadelphia}
  \state{Pennsylvania}
  \country{USA}
}

\author{Emina Torlak}
\email{emina@cs.washington.edu}
\orcid{} % chktex 8
\affiliation{%
  \institution{Amazon Web Services}
  \city{Seattle}
  \state{Washington}
  \country{USA}
}

\author{Michael Hicks}
\email{mwh@cs.umd.edu}
\orcid{} % chktex 8
\affiliation{%
  \institution{Amazon Web Services}
  \city{Arlington}
  \state{Virginia}
  \country{USA}
}

\renewcommand{\shortauthors}{Cutler et al.}

% Draft title
\title{Improving the Stability of Type Soundness Proofs in Dafny}

\begin{abstract}
  In this extended abstract, we present a systematic technique for
  structuring type soundness proofs in Dafny to improve proof stability. As a case
  study, we apply the method to proving type soundness for a small expression
  language, and demonstrate empirically how it improves resource usage metrics
  known to correlate with stability. Our method can scale to realistic proofs,
  as demonstrated by its use in the type soundness proof of the Cedar
  Language.
\end{abstract}

\keywords{}

\begin{document}
\maketitle

\section{Introduction}
Type soundness proofs for strongly-typed programming languages are often carried
out in type-theory based proof assistants like Coq or Agda. However, recent
small-scale experiements \cite{mayer-blog-post} and large-scale mechanized
metatheory developments \cite{cedar-dafny}.  have shown that Dafny is capable
enough as a proof assistant to encode type soundness proofs. In these
developments, Dafny serves both as the host language in which the target
language can be implemented, and a program verifier in which the mechanized
proof can be completed.

The usual way that a target language semantics is implemented in Dafny
is by way of a definitional interpreter \cite{definitional-interpreters}, a function \texttt{eval} which takes an
an abstract syntax tree \texttt{e} which represents a term in the target language, and returns either an error,
or the value \texttt{v} the term evaluates to. In principle, this implements some abstract semantics defined
by inference rules on paper: a semantic judgment $\texttt{e} \downarrow \texttt{v}$ holds if and only if \texttt{eval(e) = Ok(v)}.

The other half of a language implementation is a typechecker: a boolean function \texttt{check} which again takes a term \texttt{e}
and a type \texttt{t}, and returns true or false if \texttt{e} has type \texttt{t}. Again, this implements some abstract typing
judgment $\vdash \texttt{e} : \texttt{t}$, which is defined by way of inference rules on paper. 

The goal of a type soundness proof is then to demonstrate that well-typed
programs do not go wrong:
$$\vdash \texttt{e} : \texttt{t} \implies \exists \texttt{v}. \left(\texttt{e} \downarrow \texttt{v} \wedge \texttt{v} : \texttt{t}\right)$$
for some value \texttt{v} of type \texttt{t}. When proven
on paper, this proof proceeds by an induction on the typing derivation of
$\vdash \texttt{e} : \texttt{t}$. For each rule defining the typing judgment, we
prove that if the premises do not go wrong, then the conclusion also does not go
wrong. Meanwhile, a \emph{mechanized} proof of this fact in Dafny takes the form
of a lemma which requires \texttt{check(e,t)}, and ensures that \texttt{eval(e)
= Ok(v)} and that \texttt{v} has type \texttt{t}. The proof of this lemma
necessarily looks like a pattern-match on the term \texttt{e}, making recursive
calls on the subterms to establish the inductive hyoptheses. For syntax-directed type systems
of the sort we will consider in this extended abstract\footnote{Strictly speaking, this is not a restriction of our technique. By far the most common way to implement a non-syntax-directed type system is to build a syntax-directed version, prove the two versions equivalent, and then implement the syntax-directed one.},
the structure of these two kinds of proofs are the same: each case of the pattern match encodes the proof for the corresponding typing rule for the corresponding syntactic form.

The size of a type soundness proof in Dafny grows linearly with the
size of the language; more language features and more expressions mean more
cases.  This can make it hard to ensure that the proof is \emph{stable}.  Unlike
mechanized metatheory developments in tools backed by a type-theoretic proof
checker, Dafny's SMT-backed verification can be \emph{unstable}: the result of
verifying the type soundness theorem can change, from verified to unverified, seemingly at random. 
Unfortunately, folowing the proof structure described above leads to very unstable proofs in Dafny. Anecdotally, we have
found it impossible to scale this kind of proof to anything approximating a realistic language without encountering enormous
proof instability barriers.

In this extended abstract, we give a recipe for mechanized type soundness proofs
in Dafny which ensures proof stability.  By giving partial specifications of the
evaluator and type checker, and proving type soundness relative to those
specifications, we can eliminate much of the variance in resource usage.  Our
technique also scales to realistic-size languages, as witnessed by its use to
verify the type soundness proof of the Cedar Language \cite{cedar}.

In Section~\ref{sec:sts}, we present the technique by walking through a traditional-style proof of type soundness in Dafny.
Then in Section~\ref{sec:eval}, we evaluate the technique comparing empirical metrics of proof stability against other methods of structuring type soundness proofs, and other ways of writing a typechecker.
Lastly, in Section~\ref{sec:discussion}, we discuss how our technique has been deployed in practice, speculate about how it can be applied to even more complex languages including Cedar,
and discuss the limitations and drawbacks.


\section{Stable Type Soundness Proofs}
\label{sec:sts}
For a running example, let's consider the first-order expression language we
use for our evaluation in Section~\ref{sec:eval}. The full language definition, as well as the
interpreter and all of the different typecheckers we evaluate can be found in the attached supplementary materials. The language has
types for integers, booleans, and records, all given as the Dafny type
\texttt{Ty}.  For terms, we have variables (represented as strings, since the
langauge has no binders), addition, subtraction, division, and, or,
conditionals, and record expressions and projections. These are all represented
by a Dafny ADT \texttt{Term}, which has a variant for each syntactic form.
To demonstrate how the language works, and how our technique applies, we focus
on just the behavior of the addition operation.

The language's semantics is implemented by a program \texttt{eval(env,e)} which takes an environment \texttt{type
Env = map<string,Val>} mapping variables to their values and a term, and returns
either (a) the value the term evaluates to, or (b) an evaluation error, which can be a
division by zero error or a runtime type error.

The function header for the \texttt{eval} function is shown in Figure~\ref{fig:eval}.
The function \texttt{evalInt} is a helper that calls \texttt{eval}, and then pattern matches on the result,
returning \texttt{n} if it was \texttt{IntVal(n)}, and throwing a runtime type error otherwise.

\begin{figure}
\begin{dafny}
function eval(env : Env, e : Term) : Result<Val,EvErr>
{
  match e {
    ...
    case Add(e1,e2) =>
      var n1 :- evalInt(env,e1);
      var n2 :- evalInt(env,e2);
      Ok(IntVal(n1 + n2))
  }
}
  \end{dafny}
  \caption{The \texttt{eval} function, with the case for \texttt{Add}}
  \label{fig:eval}
\end{figure}


Terms are typed in a context \texttt{type Ctx = map<string,Ty>} mapping variables to their types,
with a typechecking implemented by a program \texttt{check(ctx,e,t)} which calls an inference function \texttt{infer}, and then checks if the result is equal to \texttt{t}.
Code for \texttt{check} and the \texttt{Add} case for \texttt{infer} are shown in Figure~\ref{fig:infer}, where
\texttt{inferIntTy} calls \texttt{infer}, and throws a type error if the result
is anything but \texttt{IntTy}.

\begin{figure}
\begin{dafny}
function check(ctx : Ctx, e : Term, t : Ty) : Result<(),TckErr> {
  var t' :- infer(ctx,e);
  if t == t' then Ok(()) else Err(TckErr)
}

function infer(ctx : Ctx, e : Term) : Result<Ty,TckErr> {
  match e {
    ...
    case Add(e1,e2) =>
      var _ :- inferIntTy(ctx,e1);
      var _ :- inferIntTy(ctx,e2);
      Ok(IntTy)
    }
  }
\end{dafny}
\caption{The \texttt{check} and \texttt{infer} functions, with the case for \texttt{Add}}
\label{fig:eval}
\end{figure}



\subsection*{A First Cut}

For this language, type soundness means the following. If \texttt{e} checks against \texttt{t} in context
\texttt{ctx} and the environment
\texttt{env} agrees with the context \texttt{ctx},
then \texttt{e} either evaluates under the environment \texttt{env} to a value of type \texttt{t}, or it results
in a division by zero error (but not a runtime type error). 
This lemma is encoded in dafny as the \texttt{sound} lemma in Figure~\ref{fig:sound},
along with auxiliary definitions: \texttt{envHasCtx} is the predicate saying that the environment agrees with the context,
and \texttt{isSafe(env,e,t)} encodes the conclusion of the theorem.

\begin{figure}
\begin{dafny}
lemma sound(env : Env, ctx : Ctx, e : term, t : Ty)
  requires envHasCtx(env,ctx)
  requires check(ctx,e,t).Some?
  ensures isSafe(env,e,t)
{
  match e {
    ...
    case Add(e1,e2) =>
      sound(env,ctx,e1,IntTy);
      sound(env,ctx,e2,IntTy);
  }
}

predicate envHasCtx(env : Env, ctx : Ctx) {
  forall x :: x in ctx ==>
    x in env &&
    valHasType(env[x],ctx[x])
}

predicate isSafe(env : Env, e  : Term, t : Ty){
  (eval(env,e).Ok? && valHasType(eval(env,e).value,t))
  ||
  (eval(env,e).Err? && eval(env,e).error == DivByZero)
}
\end{dafny}

  \caption{The \texttt{sound} lemma with the case for \texttt{Add}, and auxiliary definitions}
\end{figure}


To prove the \texttt{sound} lemma, we induct on the term \texttt{e}.
The case of \texttt{sound} for \texttt{Add(e1,e2)} is illustrative.
We make two recursive calls to the lemma to introduce the inductive hypotheses,
and the solver takes care of the rest. 

The reasoning that the solver takes care of under the hood is somewhat involved.
Getting from the assumption \texttt{check(ctx,Add(e1,e2),t).Some?} to
the IH preconditions \texttt{check(ctx,e1,IntTy).Some?} and
\texttt{check(ctx,e2,IntTy).Some?} requires reasoning about the (potentially complex) code for
\texttt{check} and \texttt{infer}. Similarly, getting from the IH results \texttt{isSafe(env,e1,IntTy)} and
\texttt{isSafe(env,e2,IntTy)} to the conclusion
\texttt{isSafe(env,Add(e1,e2),t)} requires reasoning through the code for \texttt{eval} to determine the possible
ways \texttt{Add(e1,e2)} evaluates when \texttt{e1} and \texttt{e2} might either evaluate to values or
raise division-by-zero errors.

\subsection*{Finding Stability}
Unfortunately, as we'll see in Section~\ref{sec:eval}, this proof approach is
not stable. We realized this while attemping to scale this style
of proof up to a realistic language. With enough operations and language
features, the proof resource usage varies wildly between runs,
enough that the verification will fail at random. At its core, this
instability arises from all of the afformentioned unguided reasoning the solver has to do about the
code of the typechecker and evaluator,
to get from the premise of the lemma to the premises of the IH, and then
from the conclusions of the IH to the conclusion of the case.
A patchwork solution is to add guidance in the form of assertions around the
recursive calls, to spell out the intermediate steps more directly.

The common folklore solution to dealing with proof instability is to specify the
functions involved in the proof, and then make the functions themselves opaque.
This way, the solver can only interact with the functions though the
specification, cutting down the search space of possible proofs and hence
improving stability.  Moreover, if the specifications are written in the form of
separate lemmas, the programmer can control how and when different parts of the
function's sepcification is revealed to the solver. In this case, the dilemma is
that it's not at all clear which functions to make opaque, and how we should
specify them.

Inspiration comes from noticing that \texttt{sound} depends only on
facts about the safety predicate \texttt{isSafe}, and not directly
on facts about evaluation. In fact, the only facts it needs about \texttt{isSafe}
are that in every case, the results of the IH calls jointly imply the conclusion.
To illustrate, the addition case for type soundness holds
for \emph{any} safety predicate \texttt{isSafe} --- even an opaque one --- which has the property
that \texttt{isSafe(env,e1,IntTy)} and \texttt{isSafe(env,e2,IntTy)}
together imply \texttt{isSafe(env,Add(e1,e2),IntTy)}.


\begin{figure}
\begin{dafny}
lemma addIsSafe(env : Env, e1 : Expr, e2 : Expr)
  requires isSafe(env,e1,IntTy)
  requires isSafe(env,e2,IntTy)
  ensures isSafe(env,Add(e1,e2),IntTy)
{reveal isSafe(); ...}
\end{dafny}
  \caption{Add Compatibility Lemma}
  \label{fig:add-is-safe}
\end{figure}

This lemma, shown as \texttt{adIsSafe} in Figure~\ref{fig:add-is-safe} is a kind
of ``compatibility lemma'', stating that safe terms can be built from smaller
safe terms.  The upshot from this is that if we make the \texttt{isSafe}
predicate opaque, the \texttt{Add} case of the soundess theorem goes through
with only minor modification, adding a call to \texttt{addIsSafe} after the uses
of IH.

\begin{dafny}
lemma sound(env : Env, ctx : Ctx, e : term, t : Ty)
  requires envHasCtx(env,ctx)
  requires check(ctx,e,t).Some?
  ensures isSafe(env,e,t)
{
  match e {
    ...
    case Add(e1,e2) =>
      sound(env,ctx,e1,IntTy);
      sound(env,ctx,e2,IntTy);
      addIsSafe(env,e1,e2);
  }
}
\end{dafny}

One way of thinking about the \texttt{addIsSafe} lemma is that it says that the
safety predicate \emph{interprets the typing rule} for addition. By by replacing
all instances of the typing judgent in the rule for Addition that the case of \texttt{infer}
for \texttt{Add(e1,e2)} implements, we arrive at the required \texttt{addIsSafe} compatibility lemma:
this is demonstrated in Figure~\ref{fig:tck-to-lemma}.  In short, all that's required for a given safety predicate to hold for every well-typed term is
that it interprets every typing rule in the language. This suggests the following technique:

\begin{figure}
  \begin{mathpar}
  \infer{\texttt{ctx} \vdash \texttt{e1} : \texttt{IntTy} \\ \texttt{ctx} \vdash \texttt{e2} : \texttt{IntTy}}
  {
    \texttt{ctx} \vdash \texttt{Add(e1,e2)} : \texttt{IntTy}
  }
  \\
  \mprset { fraction ={\cdot \cdots \cdot}}
  \infer{\texttt{isSafe(env,e1,IntTy)} \\ \texttt{isSafe(env,e1,IntTy)}}
  {
    \texttt{isSafe(env,Add(e1,e2),IntTy)}
  }
  \end{mathpar}
  \caption{Addition Typing Rule, and the Corresponding Compatibility Lemma}
  \label{fig:tck-to-lemma}
\end{figure}

\begin{enumerate}
  \item Make the safety predicate opaque, preventing the solver from directly reasoning about the interpreter in the type safety proof.
  \item Prove ``safety compatibility lemmas'' for every typing rule, demonstrating that if the safety predicate holds of all the premises, it holds of the conclusion.
  \item Write the type soundness proof, inserting calls to each case's corresponding compatibility lemma, just after the recursive calls to IH.
\end{enumerate}

\subsection*{Going Further with Inversion Lemmas}

Even with this modification, the type soundness proof still requires the solver to
do complex reasoning about the code of the typechecker. To eliminate this unguided
reasoning, we must find a way to specify the typechecker enough to make it opaque,
and have the main soundness proof refer only to the specification lemmas.

Our solution again comes from an analysis of what the solver needs to know about
\texttt{check} in each case. In the \texttt{Add(e1,e2)} case, the solver must
reason from \texttt{check(ctx,Add(e1,e2),t).Ok?} to \texttt{check(ctx,e1,IntTy).Ok?} and
\texttt{check(ctx,e2,IntTy).Ok?} for the preconditions of the IH to hold, essentially
running the code of \texttt{infer} and \texttt{check} in reverse.  This kind of reasoning
corresponds to what's known in the type systems literature as \emph{inversion
principles}: theorems that state that if the a compound syntactic form (like
\texttt{Add(e1,e2)}) is well-typed, then its component forms (here, \texttt{e1}
and \texttt{e2}) are well-typed. Moreover, this inversion lemma, shown in Figure~\ref{fig:add-invert} is slightly stronger, saying that
the type \texttt{t} in question must have been \texttt{IntTy}.


\begin{figure}
\begin{dafny}
  lemma invertAddCheck(ctx : Ctx, e1 : Term, e2 : Term)
  requires invert(ctx,Add(e1,e2),t).Ok?
  ensures check(ctx,e1,IntTy).Ok?
  ensures check(ctx,e2,IntTy).Ok?
  ensures t == IntTy
{ reveal infer(); reveal check(); }
\end{dafny}

  \caption{Inversion Lemma for Addition}
  \label{fig:add-invert}
\end{figure}

This then allows us to make the \texttt{check} and \texttt{infer} functions opaque, and modify
the proof of the soundness theorem once more to add a call to this inversion lemma before the calls to IH.

\begin{dafny}
lemma sound(env : Env, ctx : Ctx, e : term, t : Ty)
  requires envHasCtx(env,ctx)
  requires check(ctx,e,t).Some?
  ensures isSafe(env,e,t)
{
  match e {
    ...
    case Add(e1,e2) =>
      invertAddCheck(ctx,e1,e2);
      sound(env,ctx,e1,IntTy);
      sound(env,ctx,e2,IntTy);
      addIsSafe(env,e1,e2);
  }
}
\end{dafny}

This gives us the final recipe for our technique:

\begin{enumerate}
  \item Make the safety predicate opaque, preventing the solver from directly reasoning about the interpreter in the type safety proof.
  \item Prove safety compatibility lemmas for every typing rule, demonstrating that if the safety predicate holds of all the premises, it holds of the conclusion.
  \item Make the typechecker opaque
  \item Prove inversion lemmas for every typing rule, showing that if the typechecker can compute that the concluson holds, it must also be able to compute that the premises.
  \item Write the type soundness proof, inserting calls to the inversion lemmas before IH calls, and inserting calls to the compatibility lemma after.
\end{enumerate}


\section{Evaluation}
\label{sec:eval}
In this section, we demonstrate empirically that our technique as described in Section~\ref{sec:sts}
does yield more stable type soundness proofs. As a quantative proxy for stability, we will measure
the resource usage of our proofs. If a proof is expensive, or its cost varies wildly
between runs, this can be an indicator for future proof instability \cite{stabilize},\cite{tomb}
While resource usage is the most important indicator of future proof instability, we also
base our conclusions on how much verification duration --- the wall-clock time it takes for the solver
to prove the VCs --- varies between runs.

\subsection*{Experimental Setup}
We evaluate five different typecheckers and type soundness proofs for the same
language to compare their proof stability.  The code for these typecheckers, proofs, and the language's evaluator, can be found in
the supplementary materials. The five different typecheckers, as well as the contents of the rest of the files, are described below.

\begin{itemize}
  \item (\texttt{lang.dfy}): The langauge and type definitions, common to all typecheckers and the evaluator.
  \item (\texttt{eval.dfy}): Contains the evaluator for our expressiong language. All of the type soundness proofs are relative to this evaluator.
  \item (\texttt{basic.dfy}): This is the typechecker and type safety proof described in
  the first part of Section~\ref{sec:sts}.  The typechecker uses one main method
  \texttt{infer} and some helper functions to infer a type for a term, and then checks
  that it's equal to the given type.
  \item (\texttt{unfolded.dfy}): This is the same as the typechecker in \texttt{basic.dfy}, with all of the
  syntactic sugar for \texttt{Result}-binds (\texttt{:-}) unfolded, and all of the helper
  functions inlined. We include this file to measure the degree to which adding
  layers of abstraction hurts proof stability. The proof is essentially the same as the one in \texttt{basic.dfy}.
  \item (\texttt{bidirectional.dfy}): This typechecker is written in the same style as the typechecker in \texttt{basic.dfy}. The typing relation that it impelents is slightly different,
        however, including a subtyping rule which implements record width and depth subtyping.
        The type soundness proof changes to include calls to reflexivity and transitivity lemmas about subtyping, which cannot be inferred by the solver.
  \item (\texttt{opaque-safe.dfy}):
        This file implements the first half of our technique, making the safety
        property opaque, proving compatability lemmas about the evaluator, and
        calling them in the soundness proof.  The typechecker is identical to
        the one in \texttt{bidirectional.dfy}.
  \item (\texttt{opaque-safe-invert.dfy}): This file implements the full stabilizing technique, adding
  \item (\texttt{std.dfy} and \texttt{util.dfy}): Auxiliary functions and definitions not specific to the language's evaluator or typechecker.
\end{itemize}

We run the experiment by running the built-in \texttt{dafny measure-complexity}
command on the files containing each type soundness proof, with the flags \texttt{--iterations:250} and \texttt{--log-format csv}.
Each invocation of \texttt{measure-complexity} command verifies the file $250$ times, and dumps
the verification results, durations, and resource usages to a CSV file. We then parse and analyze the CSV
file with a python script, which computes the means and standard deviations of verification duration and resource usage for the solver's task of proving correctness
of the VCs in the file's main soundness theorem.
The experiments were run on a 2023 MacBook Pro with an Apple Silicon M2 processor, and 32GB memory, runnning Dafny v4.3.0, and z3 v4.12.1.

\subsection*{Results}
The graph in Figure~\ref{fig:cost-results} shows the mean verification \emph{resource usage} (and standard deviation, with whiskers), over the 250 verifications of each soundness theorem,
while the graph in Figure~\ref{fig:dur-results} shows verification \emph{durations}, in miliseconds. The two graphs tell essentially the same story.
The highest cost and highest variance proof in all cases is the soundness proof for the typechecker in \texttt{basic.dfy}. It strikes an unforuntate balance of being complicated code --- heavy use of monadic bind and lots of helper functions --- with little guidance in the proof.
The typechecker in \texttt{unfolded.dfy} is slightly cheaper to verify than the one in \texttt{basic.dfy}, but it still has enormous variance. The cost seems to be lower because of all of the inlined definitions,
meaning that the verifier must reason about fewer functions while proving soundness.
The proof for the typechecker in \texttt{bidirectional.dfy} is cheaper and has lower variance than either of the previous two. This may be because while the bidirectionality increases the complexity of the typechecking algorithm,
this complexity \emph{requires} further guidance to the verifier for it to accept the proof at all, thereby improving stability on the whole.
As expected, the two proofs using the first and second halves of our technique
are by far the best. The soundness theorem using the fully specified typechecker and safety property (\texttt{opaque-safe-invert.dfy})
is the best of all, with negligable variance in resource usage between runs of the verifier.

\begin{figure}
  \includegraphics[scale=0.5]{durgraph.png}
  \caption{Verification Duration}
  \label{fig:dur-results}
\end{figure}

\begin{figure}
  \includegraphics[scale=0.5]{costgraph.png}
  \caption{Verification Resource Usages}
  \label{fig:cost-results}
\end{figure}

\section{Discussion}
\label{sec:discussion}

\subsubsection*{Scaling It Up}
A version of this proof technique was developed for the purposes of mechanizing the proof of type soundness
of the Cedar Language \cite{cedar}
the language underlying the Amazon Verified Permissions offering \footnote{https://aws.amazon.com/verified-permissions/}.
The technique described in this abstract is currently used in the type soundness proof of the reference typechecker with respect to the reference interpreter.
In Cedar, programs express access control policies, and the evaluator outputs an authorization decision: permit or deny.
Cedar's type system is a great deal more complex than that of our toy langauge, and includes a number of advanced type system features like ocurrence typing \cite{typed-scheme} and
singleton types \cite{sing-types}.

Semantically though, Cedar is not \emph{all} that much more complex to model than the
toy language we evaluate for this extended abstract. Like our toy language,
Cedar's evaluator is simplified by the fact that it is terminating, first order, and has no binders.  Although we have not tested it, our
technique should scale to languages with nontermination --- by step-indexing the
evaluator and proving type-safety with failure to converge in $n$ steps being
considered ``safe'' \cite{step-indexing} --- and higher-order functions ---
by applying standard tricks to handle variable binding.


\subsubsection*{Manual Effort \& Benefit}
All of this benefit does come at the cost of some of the automation that Dafny
users expect. In taking possible work away from the solver to make the proof
more stable, we are necessarily creating more work for ourselves!  As we saw in
the previous section, the type soundness proofs in the last two cases spell out
many more steps along the way. Anecdotally, however, we have found it practically impossible
to scale a proof that doesn't use this technique. So while this technique does require more manual effort,
we have found it to be the only way to structure a type soundness proof that does not encounter impassable instability obstacles.

For language simpler than the one we evaluate here, the benefit of using this technique decreases. In particular, in languages
where the safety property does not include a disjunction --- safety means that every term evaluates to a value of the right type ---
the benefit shrinks dramatically. The kind of straight-line reasoning required when every term can only evaluate in one way seems to be
much easier for the verifier.


\bibliographystyle{ACM-Reference-Format}
\bibliography{refs}

% \newpage
\include{appendix}

\end{document}